# machine-learning
吴恩达机器学习系列课程
# 基础
- 监督学习：数据集的每个样本均包含正确答案（回归|分类）
# 模型描述
```
                Training Set
                    |
                    v
            Learning Algorithm
                    |
                    v
           x --->  h(x)  ---> y
```

## 数据集
- 留出法与交叉验证法适用于数据量充足的场景，但效率偏低
- 自助法适用于数据集小，难以划分的场景，但会改变数据集分布，产生估计偏差。
## 评估
- 均方误差函数：$J=\frac{1}{2m}\sum_{i=1}^m(h(x) - y)^2$，最常见的代价函数，通常能解决大部分问题
- 错误率：分类错误的样本数占总样本数的比例为错误率，精度=1-错误率

## 梯度下降
- 重复步骤$\theta := \theta - \alpha\frac{\partial{J}}{\partial{\theta}}$，其中$\alpha$为学习率，反应沿梯度下降的步幅。
- 学习率过大可能会导致代价函数不减反增的现象
$\frac{\partial{J}}{\partial{\theta}}=\frac{1}{m}\sum_{i=1}^m(h(x) - y)x$
- 特征缩放：使不同特征值取值在相近的范围内，从而使梯度下降法能够更快收敛。
- mean normalization：一种特征缩放的简单方法，令$x=\frac{x-\mu}{s}$
## 正规方程法
- 对于某些线性回归问题，正规方程法是更好的解决方案。正规方程法可以不使用特征缩放。
- 正规方程法实际上是直接令$\frac{\partial{J}}{\partial{\theta}}=0$求出最优解$\theta=(X^TX)^{-1}XY$
- 正规方程法不需要学习率，不需要迭代，但特征量n过大时矩阵的计算成本极高$O(n^3)$，当n>10000时可以考虑其他方法
- $(X^TX)$矩阵不可逆：存在多余特征或特征数量大于训练集数量
# 分类问题
## 逻辑回归
- 逻辑回归假设函数：$h_{\theta}(x)=g(\theta^Tx)$
- 使用以上假设函数会导致代价函数$J(\theta)$为非凹函数，影响梯度下降，通常将代价函数定义为$$J(\theta)=\frac{1}{m}\sum_{i=1}^{m}Cost(h(x), y),\\其中
Cost=\left\{
        \begin{matrix}
        -log(h(x)),&if&y=1\\
        -log(1-h(x)),&if&y=0
        \end{matrix}
        \right.$$
- 简化的Cost函数$Cost=-y\log{h(x)}-(1-y)\log(1-h(x))$
### 逻辑函数
- sigmoid函数：$g(z)=\frac{1}{1+e^{-z}}$，导数：$g^`(z)=g(z)(1-g(z))$
# 过拟合
- 欠拟合：高偏差，曲线不能很好地贴合样本
- 过拟合：高方差，曲线为高阶多项式，不够平滑。
- 过拟合：学习能力太强，将训练样本中的非一般特征也学到，导致泛化性下降。过拟合不可消除。
## 正则化
- 引入正则化后代价函数：$J(\theta)= J(\theta) + \frac{\lambda}{2m}\sum_{j=1}^{n}\theta^2$，通过正则化缩小某些特征值地系数
- $\lambda$为正则化参数，过大时会使$\theta$趋于0，导致欠拟合
- [正则化为什么可以缓解过拟合？](https://zhuanlan.zhihu.com/p/361181741)
## 多分类问题
- 为每一类创建一个分类器，进行多次逻辑回归训练
- 向量化可以避免循环，使训练更高效
- 引入向量化后，$\frac{\partial{J}}{\partial{\theta}}=\frac{1}{m}X^T(h(x)-y)$
- 向量化后的梯度更新公式:$\theta_j=\theta_j-\alpha[\frac{1}{m}X^T(h(x)-y)+\frac{\lambda}{m}\theta_j]$
# 神经网络
## 反向传播
- $\delta^{(n)}=(w^{(n)})^T\delta^{(n+1)}g^`(z^{(n)})$
- $\frac{\partial{J}}{\partial{\Theta^{(l)}_{ij}}}=a_j^{(l)}\delta_i^{(l+1)}$
- [反向传播推导](https://www.cnblogs.com/jsfantasy/p/12177275.html)

## 模型参数评估
- [应用机器学习的建议](https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/blob/master/markdown/week6.md)

# 支持向量机SVM
- SVM会用尽可能大的间距区分样本

# 聚类（Clustering）
## K-means算法
1. 首先选择K个随机的点，称为聚类中心（cluster centroids）；

2. 对于数据集中的每一个数据x，按照x到聚类中心的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。

3. 计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置。
4. 重复步骤2-4直至中心点不再变化。
## 优化目标
- k-均值最小化问题，即最小化所有数据点到其所关联的聚类中心的距离和（代价函数|畸变函数）：$J=\frac{1}{m}\sum_{i=1}^{m}{|x_i-\mu_{c_i}|^2}$
- k-均值迭代总是使代价J减小
## 随机初始化
- k均值算法的一个问题：停留在一个局部最小值。这取决于初始化情况。
- 解决：由训练数据中选择k个数据作为聚类中心初始化，并进行多轮k均值算法，选择代价J最小的结果。
- 该方法在k较小时(2--10)可行
## 聚类数选择
没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的。

# 降维
## 动机
- 数据压缩：2维-->1维，3维-->2维
- 便于可视化
## 主成分分析算法（PCA）
- 找到一个向量，把所有数据投影到该向量时，投射平均均方误差能尽可能地小。投射误差是从特征向量向该方向向量作垂线的长度。
- PCA n维-->k维:<br>
        1. 均值归一化<br>
        2. 计算协方差矩阵sigma=$\frac{1}{m}\sum_{i=1}^{n}(x^{(i)})(x^{(i)})^T$<br>
        3. 计算协方差矩阵的特征向量，[U, S, V]= svd(sigma)<br>
        4. 从U中选取前k个向量，$z^{(i)}=U^Tx^{(i)}$

# 异常检测
- 新的数据 $x_{test}$ 是不是异常的，即这个测试数据不属于该组数据的几率如何
- 密度估计：数据距离某组数据聚集中心越偏远，属于改组的可能性越低
- 异常检测主要用来识别欺骗。
## 算法描述
1. 求解每一特征的高斯分布$x^T_j\to\frac{1}{\sqrt{2\pi}}\exp{\frac{(x^T_j-\mu_j)^2}{2\sigma_j^2}}$
2. 计算x的概率：x每一特征值概率累积 < $\epsilon$
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 参数管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1787],\n",
       "        [0.2190]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 8), torch.nn.ReLU(), torch.nn.Linear(8, 1)\n",
    ")\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.1 参数访问\n",
    "当通过Sequential类定义模型时， 我们可以通过索引来访问模型的任意层。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.2122,  0.3054, -0.1641,  0.1500, -0.3168,  0.0388,  0.1925, -0.1025]])), ('bias', tensor([0.0189]))])\n"
     ]
    }
   ],
   "source": [
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1.1 目标参数\n",
    "参数是复合的对象，包含值、梯度和额外信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.0189], requires_grad=True)\n",
      "tensor([0.0189])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)          # 取出bias实例\n",
    "print(net[2].bias.data)     # 取出bias的值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1.2 一次性访问全部参数\n",
    "递归整个树来提取每个子块的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n",
      "tensor([0.0189])\n"
     ]
    }
   ],
   "source": [
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])\n",
    "print(net.state_dict()['2.bias'].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1.3. 从嵌套块收集参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (block0): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block1): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block3): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Linear(4, 8), torch.nn.ReLU(),\n",
    "        torch.nn.Linear(8, 4), torch.nn.ReLU()\n",
    "    )\n",
    "def block2():\n",
    "    net = torch.nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block{i}', block1())\n",
    "    return net\n",
    "rgnet = torch.nn.Sequential(block2(), torch.nn.Linear(4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.0364, -0.4430, -0.0118, -0.4868, -0.0370,  0.0181,  0.4941,  0.0860],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(rgnet[0][1][0].bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.2. 参数初始化\n",
    "默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵\n",
    "### 5.2.2.1. 内置初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0086,  0.0133,  0.0060,  0.0045],\n",
       "         [-0.0008, -0.0212,  0.0003,  0.0031],\n",
       "         [ 0.0183, -0.0003,  0.0064, -0.0015],\n",
       "         [-0.0028,  0.0152,  0.0014,  0.0048],\n",
       "         [-0.0241,  0.0154, -0.0002,  0.0225],\n",
       "         [ 0.0105,  0.0059,  0.0022, -0.0039],\n",
       "         [-0.0096, -0.0134,  0.0099,  0.0013],\n",
       "         [-0.0193,  0.0093, -0.0027, -0.0200]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == torch.nn.Linear:\n",
    "        torch.nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        torch.nn.init.constant_(m.bias, val=0)\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data, net[0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对某些块应用不同的初始化方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1841, -0.0263, -0.3763, -0.4992])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def xavier(m):\n",
    "    if type(m) == torch.nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight) # Glorot初始化\n",
    "def init_42(m):\n",
    "    if type(m) == torch.nn.Linear:\n",
    "        torch.nn.init.constant_(m.weight, 42)\n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2.2. 自定义初始化\n",
    "实现了一个my_init函数来应用到net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init ('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "Init ('weight', torch.Size([1, 8])) ('bias', torch.Size([1]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000,  0.0000, -0.0000, -0.0000],\n",
       "        [ 8.6211, -8.8919,  0.0000,  6.6446],\n",
       "        [ 7.6325, -0.0000,  5.1875, -8.0344],\n",
       "        [-0.0000, -0.0000,  0.0000, -0.0000],\n",
       "        [-6.7355, -7.8584,  9.2899,  6.2489],\n",
       "        [-5.1661, -7.0998, -8.3577, -5.6626],\n",
       "        [ 0.0000, -0.0000,  0.0000, -7.4722],\n",
       "        [-0.0000, -0.0000,  0.0000, -5.7442]], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == torch.nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape) for name, param in m.named_parameters()])\n",
    "        torch.nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "net.apply(my_init)\n",
    "net[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.3. 参数绑定\n",
    "在多个层间共享参数： 我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "shared = torch.nn.Linear(8, 8)      # 共享层\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 8), torch.nn.ReLU(),shared, torch.nn.ReLU(), shared, torch.nn.ReLU(), torch.nn.Linear(8, 1)\n",
    ")\n",
    "net(X)\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bf896dc9a96a1f05858648d8e08d4485cbbfeec133930887cfa661edfd77fd61"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
